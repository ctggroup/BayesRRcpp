---
title: "small manual"
author: "Daniel Trejo Banos"
date: "05/12/2017"
output:
  html_document: default
  pdf_document: default
---

## Current Bayes R implementation

The current package prototype is an implementation of Bayes R (citation ). It was developed in C++ 11.0 with interface to R through Rcpp. For numeric and matrix computations the library Eigen was used, along with it's R interface. The algorithm runs a chain  of BayesR using two threads, a sampling thread that computes a sample of the Markov chain and stores it in a concurrent queue, and a consumer thread that reads samples from the queue and saves them into a csv file.


Having said that, proceed as following after having cloned the repository, go to the project folder and run:

 R CMD INSTALL --no-multiarch --with-keep.source BayesRRcpp

alternatively from Rstudio open the file BayesRRcpp.Rproj

then in the upper right pannel, on the tab "build" press the button "build and reload"

It may be necessary to install the packages Rcpp and RcppEigen.

if build was succesful, we will be to use the command

```{r}
library(BayesRRcpp)

```



The code we are interested in optimize is in the file

BayesRRcpp/src/BayesRv2.cpp

Then for the standard BayesR model we have the function

```{bash,eval=F}
BayesRSamplerV2(std::string outputFile, int seed, int max_iterations, int burn_in, int thinning, Eigen::MatrixXd X, Eigen::VectorXd Y,double sigma0, double v0E, double s02E, double v0G, double s02G, Eigen::VectorXd cva)
```

where 

outputFile = File in which post burn_in samples will be saved. A dedicated thread is in charge of saving the samples as their are being produced using a concurrent queue, as such the memory overhead is expected to be low. The number of saved samples will be max_iterations-burn_in. I haven't put much effort on validating the file pahts and so on, for now I recommend suing only the file name, as such the output will be saved in the directory returned by getwd().

seed= random seed; not functional for now

max_iterations= number of samples

burn_in= burn in samples, it has to be an integer lower than max_iterations. 

thinning= thinning regime; not implemented yet

X= matrix of markers (columns) and individuals(rows)

Y= matrix of phenotypes (columns) and individuals(rows); if Y has more than one column, use the function BayesRSamplerM instead (which is slower).

sigma0 = variance of the prior over the random intercept, $\mu \sim \mathcal{N}\left(0,\sigma_0\right) $. Set to a weakly informative value of 0.01, if set to 0 then the prior becoms a noninformative uniform distribution.

v0E,s02E=  degrees of freedom and variance of the prior scaled inverse chi square distribution over $\sigma_{E}$, set to a weakly informative value of 0.01

v0G,s02G=  degrees of freedom and variance of the prior scaled inverse chi square distribution over $\sigma_{G}$, set to a weakly informative value of 0.01

cva= vector containing the component variances.

You can execute the following script to test the performance of the model.

```{r,message=F,warning=F}
library(BayesRRcpp)

MT = 6000
N = 4000
M1 = 900
M2= 100
X <- matrix(rbinom(n=MT*N, 2, .5), ncol=MT)
b <- c(rnorm(M1,0,sqrt(0.01/(M1))),rnorm(M2,0,sqrt(0.49/(M2))),rep(0,(MT-(M1+M2))))
g <- scale(X) %*% b; var(g)
e <- rnorm(N,0, sd=sqrt(var(g)))
y <- g+e
BayesRSamplerV2("test.csv",1, 15000, 10000,5,scale(X), scale(y),0.01,0.01,0.01,0.01,0.01,c(0.0001,0.001,0.01))
library(readr)
tmp<-read.csv("test.csv")
       
plot(b,sd(y)*colMeans(tmp[,grep("beta",names(tmp))]))
lines(b,b)
abline(h=0)
var(g)
mean(tmp$sigmaG)
1-var(g)
mean(tmp$sigmaE)
         
```

the sampler outputs a matrix whose columns are

beta= random effects $\beta$

sigmaG= genetic effects variance $\sigma_{G}$

components= component of the mixture from which each $\beta$ was drawn from.

sigmaE = residuals variance $\sigma_{E}$

pi= mixing proportions $\pi$

mu= random intercept parameter $\mu$

sigmaG = explained variance as computed: $\mathrm{var} \left(X\beta\right)$ , for each sample of $\beta$ 

each row contains a post burn_in set of samples for the variables.

## Installation on cluster

to install the package on the vital-it cluster:

go to your home directory (~/) and create a directory for the repository, then use git (already installed as part of vital it)

```{bash,eval = F}
git clone https://github.com/ctggroup/BayesRRcpp.git
```

now change create a directory in your home directory (if it doesnt exist)

```{bash,eval=F}
mkdir ~/.R
```

and create a Makevars file telling R to compile using  C++11 and support for multi threading

```{bash,eval=F}
echo 'CXXFLAGS += -march=native -O3 -std=c++11 -fopenmp'> ~/.R/Makevars
```

finally change to the PARENT DIRECTORY in which BayesRRcpp is contained, for example, if BayesRRcpp is in ~/repo/BayesRRcpp change to ~/repo/

there (if in vital-it) execute

```{bash,eval=F}
module add R
```

now you can start your R session

```{bash, eval=F}
R

```


Given that bayes Rcpp requires packages "Rcpp" and "RcppEigen" we are going to install them. We wont be able to install them globally but R will ask you to install them locally in your home directory

```{r,eval=F}
install.packages("Rcpp")
install.packages("RcppEigen")

```

close your R session, and exectue the following command in the same directory your BayesRRcpp folder is

```{bash,eval=F}
R CMD build BayesRRcpp
R CMD INSTALL --preclean --no-multiarch --with-keep.source BayesRRcpp
```

and now, assuming the build and installation succeeded you will be able to call BayesRcpp directly as a library

```{r,eval=F}
library(BayesRRcpp)
```


## Installation on MAC

in order to compile openMP programs on a mac we require to install llvm and its clang compiler. We will use homebrew:

https://brew.sh/index_en.html

After installing homebrew we will use it to install llvm (it will take a while):

```{bash,eval=F}
brew install --with-toolchain llvm
```

Now in order to use the new clang compiler in R studio it is necessary to create the file Makevars in your home directory

```{bash,eval=F}
mkdir ~/.R/
vi Makevars
```

The file must contain the following lines 

```{bash,eval=F}
CXX="/usr/local/opt/llvm/bin/clang"
CPPFLAGS +=  -std=c++11 -fopenmp -idirafter /Applications/Xcode.app/Contents/Developer/Platforms/MacOSX.platform/Developer/SDKs/MacOSX.sdk/usr/include/
LDFLAGS+= -L/usr/local/opt/llvm/lib
```

CXX flag tell us to use the llvm clang compiler
CPPFLAGS tell us to use C++11 openmp and the header files from Xcode.
LDFLAGS tell us to use the lib files from llvm.

hopefully set up wont vary much depending on your OSX version.

## Desired behaviour

Currently our implementations depends on storing the matrix X and Y on memory. If we wish to run many chains in parallel each chain has a copy of both matrices. For the data sets we are interested (N>5000,M>500000), this amounts to huge space in memory. 

We wish a function:

```{bash,eval=F}
BayesRSamplerV2(std::string outputFile, int seed, int max_iterations, int burn_in, int thinning, std::string XmatrixFile, std::string YmatrixFile,double sigma0, double v0E, double s02E, double v0G, double s02G,Eigen::VectorXd cva)
```

- Instead of loading the matrix X on memory we rely on a memory mapped file. The user would give the name of the file containing matrix X in comma-separated .csv files.
- Matrix Y is only used once at the begining of sampling, so ideally it should be loaded into memory and freed after sampling starts.
- If runnig multiple chains in parallel they should access the same memory mapped file, thus i/o sincronization may be needed (this requirement is not so important, we wish to save memory consumption so there may be other soulutions)
- We usually work with the scaled and centered X matrix, that means that X each column of X should have its mean substracted and the result divided by the column's standard deviation. this can be performed during sampling, but we would need to compute the columns mean and sd before sampling and storing the values in memory (not so important as this can be done as preprocessing, but would be very desirable)
- In the source code, the matrix X is called in the following lines before sampling

```{rcpp,eval=F}
xsquared=X.colwise().squaredNorm();
```
where the squared norm of each column is computed and stored in an array. this could be performend in parallel, or using the eigen's templated algorithms. Either way this would need to be performed over the memory mapped file.

- the matrix X is used in the following lines during sampling

```{rcpp,eval=F}
     y_tilde= epsilon.array()+(X.col(marker)*beta(marker,0)).array();//now y_tilde= Y-mu-X*beta+ X.col(marker)*beta(marker)_old

    num=(X.col(marker).cwiseProduct(y_tilde)).sum();
    
     epsilon=y_tilde-X.col(marker)*beta(marker,0);

```

in the code above, the current column is given by the variable "marker", which is extracted from a vector containing a random shuffle of the column indexes.

The main goal would be to perform these operations using the memory mapped file instead of reading matrix X from memory, other options would be to have a buffer of columns for reading these columns. The desirable implementation would achieve a balance between processing time and memory consumption.




